{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will continue using other cleaning processes to ensure the data used for my models is up to standard regarding data quality.I will focus on modifying the content of columns as well as identifying and adding features for my model. \n",
    "\n",
    "This notebook is also the introduction of various NLP (Natural Lanaguage Processing) techniques to prepare our project for the machne learning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Notebook Contents__\n",
    "\n",
    "__3.1__ Binarize Column\n",
    "\n",
    "__3.2__ Train_Test_Split\n",
    "\n",
    "__3.3__ Vectorization\n",
    "\n",
    "__3.4__ Saving Output for Modelling\n",
    "\n",
    "__3.5__ Closing Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Imports\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's proceed to load the cleaned dataframe from the loading notebook for pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "cleandf = joblib.load('../data/cleandf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I aim to create a final dataset which is ready for modelling. This can be achieved with a very simple step:\n",
    "\n",
    "- Binarize target column (recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.1 Binarize column__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step would be to encode our target column so it is suitable for modelling. Models do not work well with text data, so our aim is to convert the data into a numerical format. In this ase of our classififcation model, we are converting `yes` to 1 and `no` to 0 from our original `recommended` column.\n",
    "\n",
    "Once that is complete, we will then drop the `recommended` column as this would be the equivalent to a duplicate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf['recommended_class'] = cleandf['recommended'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>overall</th>\n",
       "      <th>author</th>\n",
       "      <th>customer_review</th>\n",
       "      <th>cabin</th>\n",
       "      <th>seat_comfort</th>\n",
       "      <th>cabin_service</th>\n",
       "      <th>food_bev</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>ground_service</th>\n",
       "      <th>value_for_money</th>\n",
       "      <th>recommended</th>\n",
       "      <th>review_year</th>\n",
       "      <th>review_month</th>\n",
       "      <th>recommended_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19557</th>\n",
       "      <td>Air New Zealand</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Peter Somerville</td>\n",
       "      <td>have flown with over 30 airlines and air new z...</td>\n",
       "      <td>Premium Economy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.69282</td>\n",
       "      <td>5.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34278</th>\n",
       "      <td>Air France</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Robert Stork</td>\n",
       "      <td>singapore to toulouse via paris  we booked our...</td>\n",
       "      <td>First Class</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               airline  overall            author  \\\n",
       "19557  Air New Zealand      9.0  Peter Somerville   \n",
       "34278       Air France     10.0      Robert Stork   \n",
       "\n",
       "                                         customer_review            cabin  \\\n",
       "19557  have flown with over 30 airlines and air new z...  Premium Economy   \n",
       "34278  singapore to toulouse via paris  we booked our...      First Class   \n",
       "\n",
       "       seat_comfort  cabin_service  food_bev  entertainment  ground_service  \\\n",
       "19557           5.0            5.0       5.0            5.0         2.69282   \n",
       "34278           5.0            5.0       5.0            4.0         5.00000   \n",
       "\n",
       "       value_for_money recommended  review_year  review_month  \\\n",
       "19557              5.0         yes         2013             7   \n",
       "34278              5.0         yes         2017             6   \n",
       "\n",
       "       recommended_class  \n",
       "19557                  1  \n",
       "34278                  1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output check for sanity\n",
    "cleandf.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new column `recommended_class` has been created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf = cleandf.drop(['recommended'], axis=1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Text Data Pre-processing__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are looking into focusing on text data for this part of the modelling, the column (**`customer_review`**), with the text data will be our intital feature, we will later engineer some more features out of this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting features and target variable\n",
    "X = cleandf['customer_review']\n",
    "y = cleandf['recommended_class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.2 Train_Test_Split__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step in the process is to split the data into our training and test split. This ensures overfitting is reduced and our model can adapt to new data that is later introduced.\n",
    "\n",
    "The training set will be used to train our models and the test set will be used to evaluate the model.\n",
    "\n",
    "We are using a 80:20 ratio, 80% of our dataset goes into the training set and 20% goes to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can really focus on feature extraction;\n",
    "\n",
    "We will use the `nltk` library to remove stop words from our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/faisal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.3 Vectorization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our vectorizer, we chose the TF-IDF vectorizer.\n",
    "\n",
    "TF-IDF is split in two steps;\n",
    "\n",
    "- Term Frequency (TF): The number of occurences a word has in a doument\n",
    "- Inverse Document Frequency (IDF): The inverse of the number of documents the word occurs in\n",
    "\n",
    "In a nutshell, the words that tend to appear more in the document will have a scaled down count, and the words which are less common will have a scaled up count.\n",
    "\n",
    "This is great for our use case as the words less common are more likely to be related to topics, which is exactly what we aim to identify from this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bi = TfidfVectorizer(max_features= 500, min_df=25, max_df=0.80,  stop_words= stopwords, ngram_range=(2,2))\n",
    "vectorizer_uni  = TfidfVectorizer(max_features= 500, min_df=25, max_df=0.80,  stop_words= stopwords, ngram_range=(1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_demo = TfidfVectorizer(max_features= 500, max_df=0.80, stop_words=stopwords, ngram_range=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tuned some of the features of our vectorizer for our use case:\n",
    "\n",
    "We allowed 500 features to work with, this is mainly to minimise the taxing of tasks for our machine.\n",
    "\n",
    "We focused on words which appear in at least 25 reviews, but also do not appear in more than 80% of our reviews.\n",
    "\n",
    "Finally, we tuned our n_gram range to focus on bigrams. This is because we would expect unigrams to be a good predictor of positive and negative sentiment as it would maiinly be a direct connection with an adjective. Bigrams allow us to pick out topics which for our use case of this project, would work best as it would add context to our inference problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform X_train and X_test using TF-IDF Vectorizer\n",
    "\n",
    "transformed_train = vectorizer_bi.fit_transform(X_train)\n",
    "transformed_test = vectorizer_bi.transform(X_test)\n",
    "\n",
    "transformed_train_uni = vectorizer_uni.fit_transform(X_train)\n",
    "transformed_test_uni = vectorizer_uni.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_transformed_train = vectorizer_demo.fit_transform(X_train)\n",
    "demo_transformed_test =  vectorizer_demo.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demo_vectorizer_new.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer_demo, \"demo_vectorizer_new.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __3.4 Saving Outputs for Modelling__\n",
    "\n",
    "Again a very simple process, we want to convert each dataset into a dataframe. This was it ensures readability for the next person, and also  this was would prevent any shape imbalanced at the modelling stage and should run with smoothly.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained_df .pklfor bigrams\n",
    "train_df = pd.DataFrame(columns=vectorizer_bi.get_feature_names(), data= transformed_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trained_df .pkl for unigrams\n",
    "train_df_uni = pd.DataFrame(columns=vectorizer_uni.get_feature_names(), data= transformed_train_uni.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set with y variables fro bigrams for modelling\n",
    "test_df = pd.DataFrame(columns=vectorizer_bi.get_feature_names(), data= transformed_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set with y variables fro unigrams for modelling\n",
    "test_df_uni = pd.DataFrame(columns=vectorizer_uni.get_feature_names(), data= transformed_test_uni.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/train_df.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_df, \"../data/train_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/y_train.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_train, \"../data/y_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/test_df.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(test_df, \"../data/test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/y_test.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(y_test, \"../data/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/uni_trained.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_df_uni, \"../data/uni_trained.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/uni_test.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(test_df_uni, \"../data/uni_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demo_vectorizer.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer_bi, \"demo_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['uni_vec.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vectorizer_uni, \"uni_vec.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook(s), I will be perfoming some Modelling and Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faisal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
